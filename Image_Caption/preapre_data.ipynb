{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yan_Ling\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "1. Load capions, image paths, image ids from json file\n",
    "2. Data Shuffling\n",
    "3. Prepare image: resize image, convert pixels to digits\n",
    "4. Prepare annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Load capions, image paths, image ids from json file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
      "252878848/252872794 [==============================] - 36s 0us/step\n",
      "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
      " 3385204736/13510573713 [======>.......................] - ETA: 20:45"
     ]
    }
   ],
   "source": [
    "annotation_zip = tf.keras.utils.get_file('captions.zip', \n",
    "                                          cache_subdir=os.path.abspath('.'),\n",
    "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                          extract = True)\n",
    "annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
    "\n",
    "name_of_zip = 'train2014.zip'\n",
    "if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n",
    "    image_zip = tf.keras.utils.get_file(name_of_zip, \n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "    PATH = os.path.dirname(image_zip)+'/train2014/'\n",
    "else:\n",
    "    PATH = os.path.abspath('.')+'/train2014/'\n",
    "\n",
    "# annotation_file = './coco/train/captions_train2014.json'\n",
    "# images_path = './coco/train/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions=[]\n",
    "img_name_vector =[]\n",
    "image_ids = []\n",
    "for annot in annotations['annotations']:\n",
    "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "    \n",
    "    img_name_vector.append(full_coco_image_path)\n",
    "    all_captions.append(caption)\n",
    "    image_ids.append(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find annotations dataframe, if it doesn't exist, create one. If it exists, read the dataframe.\n",
    "# if not os.path.exists('annotations.csv'):\n",
    "#     # create a dataframe to store image ids, image file paths and captions\n",
    "#     annotations = pd.DataFrame({'image_id': image_ids,\n",
    "#                             'image_file': img_name_vector,\n",
    "#                             'caption': all_captions})\n",
    "#     # Save the dataframe as csv\n",
    "#     annotations.to_csv(\"annotations.csv\")\n",
    "    \n",
    "# else:\n",
    "#     annotations = pd.read_csv(\"annotations.csv\")\n",
    "#     captions = annotations['caption'].values\n",
    "#     image_ids = annotations['image_id'].values\n",
    "#     image_files = annotations['image_file'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Data Shuffling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions, img_name_vector, image_ids = shuffle(all_captions,\n",
    "                                          img_name_vector,\n",
    "                                          image_ids, \n",
    "                                          random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Prepare image: resize images, convert pixels to digits</h3>\n",
    "<p>Two methods: one would output tensor data type, the other would generate numpy array.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###------ Method 1 ------ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize pictures by tensorflow, ouput would be 'tf.tensor'\n",
    "def tf_load_img(image_path):\n",
    "    img = tf.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize_images(img, (224, 224))\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "#     img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###------ Method 2 ------ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize pictures by Keras, output would be 'numpy array'\n",
    "\n",
    "def keras_load_img(img_path):\n",
    "    img = load_img(img_path, target_size = (224,224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    img = img_to_array(img)\n",
    "    # reshape data for the model\n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "#     img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    img + preprocess_input(img)\n",
    "    # get features\n",
    "    feature = image_features_extract_model.predict(img, verbose=0)\n",
    "    return img, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EagerTensor"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=tf_load_img('loverna-journey-1053456-unsplash.jpg')\n",
    "type(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.vgg16.VGG16(include_top=False, \n",
    "                                                weights='imagenet')\n",
    "\n",
    "# image_model = tf.keras.applications.InceptionV3(include_top=False, \n",
    "#                                                 weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "TypeError: object of type 'Tensor' has no len()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2361\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2362\u001b[1;33m         \"f\", f, \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   2363\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4e7d76985457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                                 encode_train).map(tf_load_img).batch(16)\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimage_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mbatch_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_features_extract_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mbatch_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \"\"\"\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise RuntimeError(\"dataset.__iter__() is only supported when eager \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    464\u001b[0m           format(type(self)))\n\u001b[0;32m    465\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/device:CPU:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m       \u001b[0mds_variant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1668\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m     return gen_dataset_ops.batch_dataset(\n\u001b[1;32m-> 1670\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1671\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1672\u001b[0m         output_shapes=nest.flatten(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1877\u001b[0m             sparse.as_dense_types(self.output_types, self.output_classes)),\n\u001b[0;32m   1878\u001b[0m         output_shapes=nest.flatten(\n\u001b[1;32m-> 1879\u001b[1;33m             sparse.as_dense_shapes(self.output_shapes, self.output_classes)))\n\u001b[0m\u001b[0;32m   1880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1881\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2365\u001b[0m       return map_dataset_eager_fallback(\n\u001b[0;32m   2366\u001b[0m           \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2367\u001b[1;33m           output_shapes=output_shapes, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m   2368\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2369\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset_eager_fallback\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, name, ctx)\u001b[0m\n\u001b[0;32m   2389\u001b[0m         \"'map_dataset' Op, not %r.\" % output_shapes)\n\u001b[0;32m   2390\u001b[0m   \u001b[0moutput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_s\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2391\u001b[1;33m   \u001b[0m_attr_Targuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_mixed_eager_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2392\u001b[0m   \u001b[0minput_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2393\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother_arguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mconvert_to_mixed_eager_tensors\u001b[1;34m(values, ctx)\u001b[0m\n\u001b[0;32m    202\u001b[0m       t if isinstance(t, ops.EagerTensor) else ops.EagerTensor(\n\u001b[0;32m    203\u001b[0m           t, context=ctx._handle, device=ctx.device_name)  # pylint: disable=protected-access\n\u001b[1;32m--> 204\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m   ]\n\u001b[0;32m    206\u001b[0m   \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datatype_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    202\u001b[0m       t if isinstance(t, ops.EagerTensor) else ops.EagerTensor(\n\u001b[0;32m    203\u001b[0m           t, context=ctx._handle, device=ctx.device_name)  # pylint: disable=protected-access\n\u001b[1;32m--> 204\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m   ]\n\u001b[0;32m    206\u001b[0m   \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datatype_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: TypeError: object of type 'Tensor' has no len()\n"
     ]
    }
   ],
   "source": [
    "# getting the unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# feel free to change the batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                encode_train).map(tf_load_img).batch(16)\n",
    "\n",
    "for img, path in image_dataset:\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features, [batch_features.shape[0], -1, batch_features.shape[3]])\n",
    "\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Prepare annotations</h3>\n",
    "* Step 1. 分詞，為詞彙編號碼\n",
    "* Step 2. 限制字彙數量以5000為上限，而5000以外的字彙則以\"UNK\"(for unknown)去替代\n",
    "* Step 3. 將文字與號碼配對 (create a word --> index mapping)\n",
    "* Step 4. pad_sequence() 讓每個captions的長度相同\n",
    "* Step 5. Create word2idx dict (vocabulary size < 5000)\n",
    "* Step 6. Create inx2word dict (vocabulary size < 5000)\n",
    "* Step 7. Calculate TF-IDF\n",
    "* Step 8. Build vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>keras Tokenizer API</h4>\n",
    "* word_counts: A dictionary of words and their counts.\n",
    "* word_docs: A dictionary of words and how many documents each appeared in.\n",
    "* word_index: A dictionary of words and their uniquely assigned integers.\n",
    "* document_count:An integer count of the total number of documents that were used to fit the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Step 1~2 --- ##\n",
    "\n",
    "# choosing the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "\n",
    "# 建立分詞器 tokenizer\n",
    "# oov_token指定的string，會被用來取代不在字典中的詞彙\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \n",
    "                                                  oov_token=\"<unk>\", \n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "## --- Step 3 --- ##\n",
    "# Updates internal vocabulary based on a list of texts. \n",
    "# 將每個字編號，越常出現的字彙，編號越小，越不常出現的字彙，編號越大\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "# 利用 texts_to_sequence將文本中的句子，每個字都轉換成'對應的'整數編號 (word2idx)\n",
    "# Transforms each text in texts in a sequence of integers.\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "## --- Step 4 --- ##\n",
    "# padding each vector to the max_length of the captions 令每一個caption的長度相同\n",
    "# if the max_length parameter is not provided, pad_sequences calculates that automatically\n",
    "# 若沒有設定maxlen，則程式會自動計算，此處設maxlen為20\n",
    "cap_padded = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post', maxlen=20)\n",
    "\n",
    "## --- Step 5 --- ##\n",
    "# 創建詞彙數為5000的word2inx\n",
    "tokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value<5000}\n",
    "# putting <unk> token in the word2idx dictionary 把<unk> token加入字典，編號為5001\n",
    "tokenizer.word_index[tokenizer.oov_token] = top_k + 1\n",
    "# <pad>token的編號為0\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "\n",
    "## --- Step 6 --- ##\n",
    "# creating a reverse mapping (index -> word) \n",
    "\"\"\"\n",
    "上面的 tokenizer.word_index 為dict，key為字彙、value為編號\n",
    "而index_word的key為編號、value為字彙 (idx2word)\n",
    "\"\"\"\n",
    "index_word = {value:key for key, value in tokenizer.word_index.items()}\n",
    "\n",
    "## --- Step 7 --- ##\n",
    "# create the object of tfid vectorizer 創建專門計算 TF-IDF的object\n",
    "tfid_vectorizer = TfidfVectorizer('english')\n",
    "# 將資料送進TF-IDF計算器\n",
    "tfid_vectorizer.fit(train_captions)\n",
    "\n",
    "## --- Step 8 --- ##\n",
    "# collect the vocabulary items used in the vectorizer\n",
    "# 創建dictionary (包含所有的字彙)\n",
    "dictionary = tfid_vectorizer.vocabulary_.items()\n",
    "vocabulary = {key:value for key, value in dictionary if value < 5000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use following function to calculate maximun length of captions, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the max_length 計算max_length\n",
    "# used to store the attention weights 用來儲存attention的權重值\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 9. Prepare masks which is used to erase the padding part of a sentence when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = []\n",
    "for caption in train_seqs:\n",
    "    current_num_words = len(caption)\n",
    "    current_masks = np.zeros(20) # max_caption_length = 20\n",
    "    current_masks[:current_num_words] = 1.0\n",
    "    masks.append(current_masks)\n",
    "\n",
    "data = {'word_idxs': cap_padded, 'masks': masks}\n",
    "np.save('data.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_vector, cap_padded, np.array(masks), image_ids))\n",
    "\n",
    "def map_func(img_name, cap_padded, masks):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap_padded, masks, image_ids\n",
    "\n",
    "dataset.map(lambda item1, item2, item3, item4: tf.py_func(\n",
    "    map_func, [item1, item2, item3, item4],\n",
    "    [tf.float32, tf.int32, tf.float64, tf.int64], \n",
    "    ),num_parallel_calls=2)\n",
    "\n",
    "# shuffling and batching\n",
    "dataset = dataset.shuffle(10000) #BUFFER_SIZE = 10000\n",
    "dataset = dataset.repeat(5)\n",
    "dataset = dataset.batch(64)\n",
    "dataset = dataset.prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((?,), (?, 20), (?, 20), (?,)), types: (tf.string, tf.int32, tf.float64, tf.int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
