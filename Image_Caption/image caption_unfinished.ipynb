{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What is image caption?</h3>\n",
    "<p>簡單來說就是「給機器一張圖，機器會輸出一段文字來描述這張圖」</p>\n",
    "\n",
    "步驟如下:\n",
    "1. 準備圖片與文字資料\n",
    " * 圖片:load images、resize image、normalization\n",
    " * 文字:tokenizer、create dictionary、sequence padding、\n",
    " * 將資料分成train、val和test\n",
    " * 資料分段 batch size\n",
    "\n",
    "2. Model\n",
    " * CNN: feature extract、\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yan_Ling\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense,Activation, Dropout, Input\n",
    "from keras.layers import Conv2D,MaxPool2D\n",
    "from keras.utils import plot_model\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "from prepare_data import load_data, load_img, load_word\n",
    "\n",
    "train_captions, img_name_vector, image_ids = load_data()\n",
    "sentences, masks = load_word(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, list, list, list)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), len(masks), len(train_captions), len(img_name_vector), len(image_ids)\n",
    "type(sentences), type(masks), type(train_captions), type(img_name_vector), type(image_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "CNN可按照喜好使用vgg16、vgg19、resnet50、inception...等，已被train好的模型，這裡將使用vgg16。\n",
    "\"\"\"\n",
    "\n",
    "# Define CNN Model as Encoder\n",
    "1. 用tensorflow手刻模型\n",
    "2. 用Keras直接載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "image_shape = [224,224,3]\n",
    "\n",
    "kernel_size = (3,3)\n",
    "strides = (1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg16(image, batch_size, image_shape, kernel_size, strides):\n",
    "#     images=tf.placeholder(tf.float32, shape= [batch_size] + image_shape) # image_shape = [32,224,224,3]\n",
    "    conv1_1_feats = tf.layers.conv2d(images, 64, kernel_size, strides, padding ='same', \n",
    "                                     activation =tf.nn.relu, use_bias = True, \n",
    "                                     name = 'conv1_1')\n",
    "    conv1_2_feats = tf.layers.conv2d(conv1_1_feats, 64, kernel_size, strides, padding ='same', \n",
    "                                     activation =tf.nn.relu, use_bias = True, \n",
    "                                     name = 'conv1_2')\n",
    "    pool1_feats = tf.layers.max_pooling2d(conv1_2_feats, pool_size=2, strides=2, name = 'pool1')\n",
    "\n",
    "    conv2_1_feats = tf.layers.conv2d(pool1_feats, 128, kernel_size, strides, padding ='same', \n",
    "                                     activation =tf.nn.relu, use_bias = True, \n",
    "                                     name = 'conv2_1')\n",
    "    conv2_2_feats = tf.layers.conv2d(conv2_1_feats, 128,kernel_size, strides, padding ='same', \n",
    "                                     activation =tf.nn.relu, use_bias = True, \n",
    "                                     name = 'conv2_2')\n",
    "    pool2_feats = tf.layers.max_pooling2d(conv2_2_feats, pool_size=2, strides =2, name = 'pool2')\n",
    "\n",
    "    conv3_1_feats = tf.layers.conv2d(pool2_feats, 256, kernel_size, strides, padding ='same', \n",
    "                                     activation =tf.nn.relu, use_bias = True, \n",
    "                                     name = 'conv3_1')\n",
    "    conv3_2_feats = tf.layers.conv2d(conv3_1_feats, 256, kernel_size, strides, padding ='same', activation =tf.nn.relu, use_bias = True, name = 'conv3_2')\n",
    "    conv3_3_feats = tf.layers.conv2d(conv3_2_feats, 256, kernel_size, strides, padding ='same', activation =tf.nn.relu, use_bias = True, name = 'conv3_3')\n",
    "    pool3_feats = tf.layers.max_pooling2d(conv3_3_feats, pool_size=2, strides =2, name = 'pool3')\n",
    "\n",
    "    conv4_1_feats = tf.layers.conv2d(pool3_feats, 512, kernel_size, strides, padding ='same', activation =tf.nn.relu, use_bias = True, name = 'conv4_1')\n",
    "    conv4_2_feats = tf.layers.conv2d(conv4_1_feats, 512, kernel_size, strides, padding ='same', activation =tf.nn.relu, use_bias = True, name = 'conv4_2')\n",
    "    conv4_3_feats = tf.layers.conv2d(conv4_2_feats, 512, kernel_size, strides, padding ='same', activation =tf.nn.relu, use_bias = True, name = 'conv4_3')\n",
    "    pool4_feats = tf.layers.max_pooling2d(conv4_3_feats, pool_size=2, strides =2, name = 'pool4')\n",
    "\n",
    "    conv5_1_feats = tf.layers.conv2d(pool4_feats, 512, kernel_size, strides, padding ='same', activation =tf.nn.relu, use_bias = True, name = 'conv5_1')\n",
    "    conv5_2_feats = tf.layers.conv2d(conv5_1_feats, 512, kernel_size, strides, padding ='same', activation =tf.nn.relu, use_bias = True, name = 'conv5_2')\n",
    "    conv5_3_feats = tf.layers.conv2d(conv5_2_feats, 512, kernel_size, strides, padding ='same', activation =tf.nn.relu, use_bias = True, name = 'conv5_3')\n",
    "    \n",
    "    reshaped_conv5_3_feats = tf.reshape(conv5_3_feats, \n",
    "                                        [batch_size, 196, 512])\n",
    "\n",
    "    conv_feats = reshaped_conv5_3_feats\n",
    "    return conv_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>RNN Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "dim_embedding = 512\n",
    "max_caption_length = 20\n",
    "\n",
    "num_lstm_units = 512\n",
    "vocabulary_size = 5000\n",
    "\n",
    "num_ctx = 196 # 有196個context vector，每一張圖萃取出196個region，每一個region用一個vector表示\n",
    "dim_ctx = 512 \n",
    "\n",
    "fc_drop_rate = 0.5\n",
    "lstm_drop_rate = 0.3\n",
    "attention_loss_factor = 0.01\n",
    "\n",
    "fc_kernel_initializer_scale = 0.08\n",
    "fc_kernel_initializer = tf.random_uniform_initializer(\n",
    "            minval = -fc_kernel_initializer_scale,\n",
    "            maxval = fc_kernel_initializer_scale)\n",
    "\n",
    "is_train = True\n",
    "\n",
    "fc_kernel_regularizer_scale = 1e-4\n",
    "if fc_kernel_regularizer_scale > 0:\n",
    "    fc_kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = fc_kernel_regularizer_scale)\n",
    "else:\n",
    "    fc_kernel_regularizer = None\n",
    "\n",
    "# activity_regularizer = tf.contrib.layers.l1_regularizer(scale = 0.0, scope = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_feats = reshaped_conv5_3_feats\n",
    "# contexts = conv_feats\n",
    "# sentences = tf.placeholder(dtype=tf.int32, \n",
    "#                            shape=[batch_size, max_caption_length]) # 32 * 20\n",
    "# masks = tf.placeholder(dtype=tf.float32, \n",
    "#                        shape=[batch_size, max_caption_length]) # 32 * 20\n",
    "\n",
    "# last_memory = tf.placeholder(\n",
    "#     dtype=tf.float32,\n",
    "#     shape=[batch_size, num_lstm_units]) # 32 * 512\n",
    "\n",
    "# last_output = tf.placeholder(\n",
    "#     dtype=tf.float32,\n",
    "#     shape=[batch_size, num_lstm_units]) # 32 * 512\n",
    "\n",
    "# last_word = tf.placeholder(\n",
    "#     dtype=tf.int32,\n",
    "#     shape=[batch_size]) # 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"以context_mean來初始化\"\"\"\n",
    "def initialize(cont_mean):\n",
    "    context_mean = tf.layers.dropout(inputs = cont_mean, rate = fc_drop_rate, training = is_train)\n",
    "    ##fc_drop_rate = 0.5;is_train = True\n",
    "    memory = tf.layers.dense(cont_mean, units=num_lstm_units,\n",
    "                             activation = None,\n",
    "                             use_bias = True,\n",
    "                             trainable = is_train,\n",
    "                             activity_regularizer = None)\n",
    "                           \n",
    "    output = tf.layers.dense(cont_mean,\n",
    "                           units=num_lstm_units,\n",
    "                           activation=None,\n",
    "                           use_bias = True,\n",
    "                           trainable = is_train,\n",
    "                           activity_regularizer = None)\n",
    "    return memory, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "\"\"\"\n",
    "1. calculate match score\n",
    "2. put match score into softmax layer to obtain alpha (seen as probability)\n",
    "\n",
    "contexts = conv_feats\n",
    "維度為 32 x 196 x 512 \n",
    "\n",
    "output 是 last output\n",
    "維度為 32 * 512\n",
    "\"\"\"\n",
    "\n",
    "def attend(contexts, output):\n",
    "    reshaped_context = tf.reshape(contexts, [-1,dim_ctx]) # 6272 * 512\n",
    "    reshaped_context = tf.layers.dropout(reshaped_context, \n",
    "                                      rate = fc_drop_rate)\n",
    "    output = tf.layers.dropout(output, fc_drop_rate) # 32 * 512\n",
    "  \n",
    "    logits1 = tf.layers.dense(reshaped_context, \n",
    "                           units = 1,\n",
    "                           activation = None,\n",
    "                           use_bias = False)\n",
    "                            # after shape 6272 * 1\n",
    "    logits1 = tf.reshape(logits1, [-1, num_ctx]) #  32 * 196\n",
    "  \n",
    "    logits2 = tf.layers.dense(output, \n",
    "                           units = num_ctx, \n",
    "                           activation = None,\n",
    "                           use_bias = False) #  32 * 196\n",
    "    logits = logits1 + logits2 # 32 * 196\n",
    "  \n",
    "    alpha = tf.nn.softmax(logits) # 32 * 196\n",
    "  \n",
    "    return alpha # 32 * 196\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(expanded_output):\n",
    "    \"\"\" Decode the expanded output of the LSTM into a word. \"\"\"\n",
    "    expanded_output = tf.layers.dropout(expanded_output)\n",
    " \n",
    "    logits = tf.layers.dense(expanded_output,\n",
    "                               units = vocabulary_size,\n",
    "                               activation = None,\n",
    "                               name = 'fc')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare to run\n",
    "# predictions = []\n",
    "\n",
    "# alphas = []\n",
    "# cross_entropies = []\n",
    "# predictions_correct = []\n",
    "# num_steps = max_caption_length\n",
    "# last_output = initial_output # 把初始化的output當成上一步的output\n",
    "# last_memory = initial_memory # 初始化的memory當作上一步的memory\n",
    "# last_word = tf.zeros([batch_size], tf.int32) # 上一步輸出的詞彙\n",
    "\n",
    "    \n",
    "# last_state = last_memory, last_output # 上一個cell的狀態為tuple (last_memory, last_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 每一句有20個字, num_steps＝20\n",
    "# \"\"\"\n",
    "# 1. 丟進attention，得到masked_alpha值 \n",
    "# 2. 透過查找embedding_matrix，找到上一個字的word vector\n",
    "# 3. 丟進lstm\n",
    "# \"\"\"\n",
    "# for idx in range(num_steps):\n",
    "#   # Attention Mechanism\n",
    "#   with tf.variable_scope('attend', reuse=tf.AUTO_REUSE) as scope:\n",
    "\n",
    "#     alpha = attend(contexts, last_output)\n",
    "#     \"\"\"attention的第三個步驟： \n",
    "#     contexts shape == 32 * 196 * 512; \n",
    "#     alpha shape == 32 * 196\n",
    "#     alpha擴展第三個維度 ＝＝ (32*196*1)\n",
    "#     將alpha值乘上 contexts\n",
    "#     \"\"\"\n",
    "#     context = tf.reduce_sum(contexts * tf.expand_dims(alpha,2), axis = 1) # after shape 32 * 512\n",
    "    \n",
    "#     tiled_masks = tf.tile(tf.expand_dims(masks[:, idx], 1),\n",
    "#                           [1, 196])\n",
    "#     masked_alpha = alpha * tiled_masks\n",
    "#     alphas.append(tf.reshape(masked_alpha, [-1]))\n",
    "    \n",
    "#   # Embed the last word\n",
    "#   with tf.variable_scope(\"word_embedding\"):\n",
    "#     word_embed = tf.nn.embedding_lookup(embedding_matrix,\n",
    "#                                           last_word)\n",
    "  \n",
    "#   # Apply the LSTM\n",
    "#   \"\"\"\n",
    "#   將上一個字和context\n",
    "#   \"\"\"\n",
    "#   with tf.variable_scope(\"lstm\"):\n",
    "#     current_input = tf.concat([context, word_embed], 1)\n",
    "#     output, state = lstm(current_input, last_state)\n",
    "#     memory, _ = state\n",
    "\n",
    "#   # Decode the expanded output of LSTM into a word\n",
    "#   with tf.variable_scope(\"decode\", reuse=tf.AUTO_REUSE) as de_scope:\n",
    "# #       de_scope.reuse_variables()\n",
    "#     expanded_output = tf.concat([output,\n",
    "#                                    context,\n",
    "#                                    word_embed],\n",
    "#                                   axis=1)\n",
    "#     logits = decode(expanded_output)\n",
    "#     probs = tf.nn.softmax(logits)\n",
    "\n",
    "#     prediction = tf.argmax(logits, 1)\n",
    "#     predictions.append(prediction)\n",
    "\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#           labels=sentences[:, idx],\n",
    "#           logits=logits)\n",
    "#     masked_cross_entropy = cross_entropy * masks[:, idx]\n",
    "#     cross_entropies.append(masked_cross_entropy)\n",
    "\n",
    "#     ground_truth = tf.cast(sentences[:, idx], tf.int64)\n",
    "#     prediction_correct = tf.where(\n",
    "#           tf.equal(prediction, ground_truth),\n",
    "#           tf.cast(masks[:, idx], tf.float32),\n",
    "#           tf.cast(tf.zeros_like(prediction), tf.float32))\n",
    "#     predictions_correct.append(prediction_correct)\n",
    "\n",
    "#     last_output = output\n",
    "#     last_memory = memory\n",
    "#     last_state = state\n",
    "#     last_word = sentences[:, idx]\n",
    "\n",
    "#     tf.get_variable_scope().reuse_variables() \n",
    "    \n",
    "#     cross_entropies = tf.stack(cross_entropies, axis=1)\n",
    "#     cross_entropy_loss = tf.reduce_sum(cross_entropies) / tf.reduce_sum(masks)\n",
    "\n",
    "#     alphas = tf.stack(alphas, axis=1)\n",
    "#     alphas = tf.reshape(alphas, [batch_size, num_ctx, -1])\n",
    "#     attentions = tf.reduce_sum(alphas, axis=2)\n",
    "#     diffs = tf.ones_like(attentions) - attentions\n",
    "#     attention_loss = attention_loss_factor \\\n",
    "#                  * tf.nn.l2_loss(diffs) \\\n",
    "#                  / (batch_size * num_ctx)\n",
    "\n",
    "#     reg_loss = tf.losses.get_regularization_loss()\n",
    "\n",
    "#     total_loss = cross_entropy_loss + attention_loss + reg_loss\n",
    "\n",
    "#     predictions_correct = tf.stack(predictions_correct, axis=1)\n",
    "#     accuracy = tf.reduce_sum(predictions_correct) \\\n",
    "#            / tf.reduce_sum(masks)\n",
    "    \n",
    "#     contexts = contexts\n",
    "\n",
    "#     sentences = sentences\n",
    "#     masks = masks\n",
    "#     total_loss = total_loss\n",
    "#     cross_entropy_loss = cross_entropy_loss\n",
    "#     attention_loss = attention_loss\n",
    "#     reg_loss = reg_loss\n",
    "#     accuracy = accuracy\n",
    "#     attentions = attentions\n",
    "\n",
    "#     initial_memory = initial_memory\n",
    "#     initial_output = initial_output\n",
    "#     last_memory = last_memory\n",
    "#     last_output = last_output\n",
    "#     last_word = last_word\n",
    "#     memory = memory\n",
    "#     output = output\n",
    "#     probs = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_and_loss(contexts, senteces, masks):\n",
    "    last_memory = tf.placeholder(\n",
    "        dtype=tf.float32,\n",
    "        shape=[batch_size, num_lstm_units]) # 32 * 512\n",
    "\n",
    "    last_output = tf.placeholder(\n",
    "        dtype=tf.float32,\n",
    "        shape=[batch_size, num_lstm_units]) # 32 * 512\n",
    "\n",
    "    last_word = tf.placeholder(\n",
    "        dtype=tf.int32,\n",
    "        shape=[batch_size]) # 32\n",
    "    \n",
    "    with tf.variable_scope('word_embedding'):\n",
    "        embedding_matrix = tf.get_variable(shape=[vocab_size,dim_embedding],\n",
    "                                    initializer=fc_kernel_initializer,                                    \n",
    "                                    trainable=is_train,\n",
    "                                    name = 'weights')\n",
    "    # Initialize the LSTM using the mean context\n",
    "    with tf.variable_scope(\"initialize\"):\n",
    "    #     context_mean = tf.reduce_mean(conv_feats, axis=1) # after shape 32 * 512\n",
    "        initial_memory, initial_output = initialize(tf.reduce_mean(conv_feats, axis=1))\n",
    "        initial_state = initial_memory, initial_output # 32 * 512\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(\n",
    "            num_lstm_units,\n",
    "            initializer=fc_kernel_initializer)\n",
    "\n",
    "    lstm = tf.nn.rnn_cell.DropoutWrapper(lstm,\n",
    "      input_keep_prob=1.0 -lstm_drop_rate,\n",
    "      output_keep_prob=1.0 - lstm_drop_rate,\n",
    "      state_keep_prob=1.0 - lstm_drop_rate)\n",
    "    \n",
    "#     prepare to run\n",
    "    predictions =[]\n",
    "    cross_entropies = []\n",
    "\n",
    "    alphas = []\n",
    "    predictions_correct = []\n",
    "\n",
    "    num_steps = max_caption_length\n",
    "\n",
    "    last_output = initial_output\n",
    "    last_memory = initial_memory\n",
    "    last_word = tf.zeros([batch_size], tf.int32)\n",
    "\n",
    "    last_state = last_memory, last_output\n",
    "    \n",
    "    for idx in range(num_steps):\n",
    "        with tf.variable_scope('attend', reuse=tf.AUTO_REUSE):\n",
    "            alpha = attend(contexts, last_output)\n",
    "            \"\"\"attention的第三個步驟： \n",
    "            contexts shape == 32 * 196 * 512; \n",
    "            alpha shape == 32 * 196\n",
    "            alpha擴展第三個維度 ＝＝ (32*196*1)\n",
    "            將alpha值乘上 contexts\n",
    "            \"\"\"\n",
    "            context = tf.reduce_sum(contexts * tf.expand_dims(alpha,2), axis = 1) # after shape 32 * 512\n",
    "\n",
    "            tiled_masks = tf.tile(tf.expand_dims(masks[:, idx], 1),\n",
    "                                  [1, 196])\n",
    "            masked_alpha = alpha * tiled_masks\n",
    "            alphas.append(tf.reshape(masked_alpha, [-1]))\n",
    "\n",
    "      # Embed the last word\n",
    "        with tf.variable_scope(\"word_embedding\"):\n",
    "            word_embed = tf.nn.embedding_lookup(embedding_matrix,\n",
    "                                              last_word)\n",
    "\n",
    "      # Apply the LSTM\n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            current_input = tf.concat([context, word_embed], 1)\n",
    "            output, state = lstm(current_input, last_state)\n",
    "            memory, _ = state\n",
    "\n",
    "      # Decode the expanded output of LSTM into a word\n",
    "        with tf.variable_scope(\"decode\", reuse=tf.AUTO_REUSE):\n",
    "            expanded_output = tf.concat([output,\n",
    "                                       context,\n",
    "                                       word_embed],\n",
    "                                      axis=1)\n",
    "            logits = decode(expanded_output)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "\n",
    "            prediction = tf.argmax(logits, 1)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "            # Compute the loss for this step\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "              labels=sentences[:, idx],\n",
    "              logits=logits)\n",
    "            masked_cross_entropy = cross_entropy * masks[:, idx]\n",
    "            cross_entropies.append(masked_cross_entropy)\n",
    "\n",
    "            ground_truth = tf.cast(sentences[:, idx], tf.int64)\n",
    "            prediction_correct = tf.where(\n",
    "                  tf.equal(prediction, ground_truth),\n",
    "                  tf.cast(masks[:, idx], tf.float32),\n",
    "                  tf.cast(tf.zeros_like(prediction), tf.float32))\n",
    "            predictions_correct.append(prediction_correct)\n",
    "\n",
    "            last_output = output\n",
    "            last_memory = memory\n",
    "            last_state = state\n",
    "            last_word = sentences[:, idx]\n",
    "\n",
    "        tf.get_variable_scope().reuse_variables() \n",
    "        return predictions, cross_entropies # both are lists \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to change the batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                img_name_vector).map(load_img).batch(32)\n",
    "for images,path in image_dataset:\n",
    "    ctx_vectors = build_vgg16(images, batch_size, image_shape, kernel_size, strides)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((ctx_vectors, sentences, masks, image_ids))\n",
    "dataset = dataset.shuffle(10000) #BUFFER_SIZE = 10000\n",
    "dataset = dataset.repeat(5)\n",
    "dataset = dataset.batch(64)\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input filename tensor must be scalar, but had shape: [64] [Op:ReadFile]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b33d1bc6da46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_name_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_name_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mcontexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_vgg16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_entrpies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn_and_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Projects\\Image_Caption\\prepare_data.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# resize pictures by tensorflow, ouput would be 'tf.tensor'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_jpeg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    548\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Input filename tensor must be scalar, but had shape: [64] [Op:ReadFile]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch, (contexts, sentences, masks, image_ids) in enumerate(dataset):        \n",
    "        predictions, cross_entrpies = rnn_and_loss(contexts, cap_padded, masks)\n",
    "    \n",
    "        cross_entropies = tf.stack(cross_entropies, axis=1)\n",
    "        cross_entropy_loss = tf.reduce_sum(cross_entropies)/tf.reduce_sum(masks)\n",
    "\n",
    "        alphas = tf.stack(alphas, axis=1)\n",
    "        alphas = tf.reshape(alphas, [batch_size, num_ctx, -1])\n",
    "        attentions = tf.reduce_sum(alphas, axis=2)\n",
    "        diffs = tf.ones_like(attentions) - attentions\n",
    "        attention_loss = attention_loss_factor* tf.nn.l2_loss(diffs)/(batch_size * num_ctx)\n",
    "\n",
    "        reg_loss = tf.losses.get_regularization_loss()\n",
    "\n",
    "        total_loss = cross_entropy_loss #+ attention_loss + reg_loss\n",
    "\n",
    "        predictions_correct = tf.stack(predictions_correct, axis=1)\n",
    "        accuracy = tf.reduce_sum(predictions_correct)/ tf.reduce_sum(masks)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
