{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yan_Ling\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "1. Load capions, image paths, image ids from json file\n",
    "2. Data Shuffling\n",
    "3. Prepare image: resize image, convert pixels to digits\n",
    "4. Prepare annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Load capions, image paths, image ids from json file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation_zip = tf.keras.utils.get_file('captions.zip', \n",
    "#                                           cache_subdir=os.path.abspath('.'),\n",
    "#                                           origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "#                                           extract = True)\n",
    "# annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
    "\n",
    "# name_of_zip = 'train2014.zip'\n",
    "# if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n",
    "#     image_zip = tf.keras.utils.get_file(name_of_zip, \n",
    "#                                       cache_subdir=os.path.abspath('.'),\n",
    "#                                       origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "#                                       extract = True)\n",
    "#     PATH = os.path.dirname(image_zip)+'/train2014/'\n",
    "# else:\n",
    "#     PATH = os.path.abspath('.')+'/train2014/'\n",
    "\n",
    "annotation_file = 'captions_train2014.json'\n",
    "images_path = './train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions=[]\n",
    "img_name_vector =[]\n",
    "image_ids = []\n",
    "for annot in annotations['annotations']:\n",
    "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "    \n",
    "    img_name_vector.append(full_coco_image_path)\n",
    "    all_captions.append(caption)\n",
    "    image_ids.append(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find annotations dataframe, if it doesn't exist, create one. If it exists, read the dataframe.\n",
    "# if not os.path.exists('annotations.csv'):\n",
    "#     # create a dataframe to store image ids, image file paths and captions\n",
    "#     annotations = pd.DataFrame({'image_id': image_ids,\n",
    "#                             'image_file': img_name_vector,\n",
    "#                             'caption': all_captions})\n",
    "#     # Save the dataframe as csv\n",
    "#     annotations.to_csv(\"annotations.csv\")\n",
    "    \n",
    "# else:\n",
    "#     annotations = pd.read_csv(\"annotations.csv\")\n",
    "#     captions = annotations['caption'].values\n",
    "#     image_ids = annotations['image_id'].values\n",
    "#     image_files = annotations['image_file'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Data Shuffling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions, img_name_vector, image_ids = shuffle(all_captions,\n",
    "                                          img_name_vector,\n",
    "                                          image_ids, \n",
    "                                          random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Prepare image: resize images, convert pixels to digits</h3>\n",
    "<p>Two methods: one would output tensor data type, the other would generate numpy array.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###------ Method 1 ------ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize pictures by tensorflow, ouput would be 'tf.tensor'\n",
    "def tf_load_img(image_path):\n",
    "    img = tf.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize_images(img, (224, 224))\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "#     img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "#     img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###------ Method 2 ------ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize pictures by Keras, output would be 'numpy array'\n",
    "\n",
    "def keras_load_img(img_path):\n",
    "    img = load_img(img_path, target_size = (224,224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    img = img_to_array(img)\n",
    "    # reshape data for the model\n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "#     img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    img + preprocess_input(img)\n",
    "    # get features\n",
    "    feature = image_features_extract_model.predict(img, verbose=0)\n",
    "    return img, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_load_img('loverna-journey-1053456-unsplash.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.vgg16.VGG16(include_top=False, \n",
    "                                                weights='imagenet')\n",
    "\n",
    "# image_model = tf.keras.applications.InceptionV3(include_top=False, \n",
    "#                                                 weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# feel free to change the batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                encode_train).map(tf_load_img).batch(16)\n",
    "\n",
    "for img, path in image_dataset:\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features, [batch_features.shape[0], -1, batch_features.shape[3]])\n",
    "\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Prepare annotations</h3>\n",
    "* Step 1. 分詞，為詞彙編號碼\n",
    "* Step 2. 限制字彙數量以5000為上限，而5000以外的字彙則以\"UNK\"(for unknown)去替代\n",
    "* Step 3. 將文字與號碼配對 (create a word --> index mapping)\n",
    "* Step 4. pad_sequence() 讓每個captions的長度相同\n",
    "* Step 5. Create word2idx dict (vocabulary size < 5000)\n",
    "* Step 6. Create inx2word dict (vocabulary size < 5000)\n",
    "* Step 7. Calculate TF-IDF\n",
    "* Step 8. Build vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>keras Tokenizer API</h4>\n",
    "* word_counts: A dictionary of words and their counts.\n",
    "* word_docs: A dictionary of words and how many documents each appeared in.\n",
    "* word_index: A dictionary of words and their uniquely assigned integers.\n",
    "* document_count:An integer count of the total number of documents that were used to fit the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Step 1~2 --- ##\n",
    "\n",
    "# choosing the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "\n",
    "# 建立分詞器 tokenizer\n",
    "# oov_token指定的string，會被用來取代不在字典中的詞彙\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \n",
    "                                                  oov_token=\"<unk>\", \n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "## --- Step 3 --- ##\n",
    "# Updates internal vocabulary based on a list of texts. \n",
    "# 將每個字編號，越常出現的字彙，編號越小，越不常出現的字彙，編號越大\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "# 利用 texts_to_sequence將文本中的句子，每個字都轉換成'對應的'整數編號 (word2idx)\n",
    "# Transforms each text in texts in a sequence of integers.\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "## --- Step 4 --- ##\n",
    "# padding each vector to the max_length of the captions 令每一個caption的長度相同\n",
    "# if the max_length parameter is not provided, pad_sequences calculates that automatically\n",
    "# 若沒有設定maxlen，則程式會自動計算，此處設maxlen為20\n",
    "cap_padded = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post', maxlen=20)\n",
    "\n",
    "## --- Step 5 --- ##\n",
    "# 創建詞彙數為5000的word2inx\n",
    "tokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value<5000}\n",
    "# putting <unk> token in the word2idx dictionary 把<unk> token加入字典，編號為5001\n",
    "tokenizer.word_index[tokenizer.oov_token] = top_k + 1\n",
    "# <pad>token的編號為0\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "\n",
    "## --- Step 6 --- ##\n",
    "# creating a reverse mapping (index -> word) \n",
    "\"\"\"\n",
    "上面的 tokenizer.word_index 為dict，key為字彙、value為編號\n",
    "而index_word的key為編號、value為字彙 (idx2word)\n",
    "\"\"\"\n",
    "index_word = {value:key for key, value in tokenizer.word_index.items()}\n",
    "\n",
    "## --- Step 7 --- ##\n",
    "# create the object of tfid vectorizer 創建專門計算 TF-IDF的object\n",
    "tfid_vectorizer = TfidfVectorizer('english')\n",
    "# 將資料送進TF-IDF計算器\n",
    "tfid_vectorizer.fit(train_captions)\n",
    "\n",
    "## --- Step 8 --- ##\n",
    "# collect the vocabulary items used in the vectorizer\n",
    "# 創建dictionary (包含所有的字彙)\n",
    "dictionary = tfid_vectorizer.vocabulary_.items()\n",
    "vocabulary = {key:value for key, value in dictionary if value < 5000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use following function to calculate maximun length of captions, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the max_length 計算max_length\n",
    "# used to store the attention weights 用來儲存attention的權重值\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 9. Prepare masks which is used to erase the padding part of a sentence when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = []\n",
    "for caption in train_seqs:\n",
    "    current_num_words = len(caption)\n",
    "    current_masks = np.zeros(20) # max_caption_length = 20\n",
    "    current_masks[:current_num_words] = 1.0\n",
    "    masks.append(current_masks)\n",
    "\n",
    "# data = {'word_idxs': cap_padded, 'masks': masks}\n",
    "# np.save('data.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_vector, cap_padded, np.array(masks), image_ids))\n",
    "\n",
    "def map_func(img_name, cap_padded, masks):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap_padded, masks, image_ids\n",
    "\n",
    "dataset.map(lambda item1, item2, item3, item4: tf.py_func(\n",
    "    map_func, [item1, item2, item3, item4],\n",
    "    [tf.float32, tf.int32, tf.float64, tf.int64], \n",
    "    ),num_parallel_calls=2)\n",
    "\n",
    "# shuffling and batching\n",
    "dataset = dataset.shuffle(10000) #BUFFER_SIZE = 10000\n",
    "dataset = dataset.repeat(5)\n",
    "dataset = dataset.batch(64)\n",
    "dataset = dataset.prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
